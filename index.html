<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Irmak Guzey</title> <meta name="author" content="Irmak Guzey"/> <meta name="description" content="A graduate research assistant in New York University. I am interested in robot learning and computer vision. "/> <meta name="keywords" content="machine-learning, research-assistant, computer-science"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://irmakguzey.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About Me<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Irmak Guzey </h1> <p class="desc">PhD student in Computer Science, New York University (she/her)</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile_photo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile_photo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile_photo-1400.webp"></source> <img src="/assets/img/profile_photo.jpg" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="profile_photo.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>irmakguzey@nyu.edu</p> <p>New York, NY, United States</p> </div> </div> <div class="clearfix"> <p>I am currently a first year PhD student in Computatinal Intelligence, Vision, and Robotics Lab (<a href="https://wp.nyu.edu/cilvr/" target="_blank" rel="noopener noreferrer">CILVR</a>) at NYU and am lucky to be advised by <a href="https://www.lerrelpinto.com/" target="_blank" rel="noopener noreferrer">Prof. Lerrel Pinto</a>.</p> <p>I had my master’s degree from New York University, Computer Science at Courant Institute of Mathematical Sciences. I was awarded with a Fulbright scholarship in 2020.</p> <p>My research mostly focus on <em>Policy Learning</em> and <em>Representation Learning</em>. I enjoy working on tackling different tasks in Manipulation and Navigation in Robotics.</p> <p>On top of my research, with my interest in Robotics and Machine Learning, I worked as a full-time robotics engineer in <a href="https://dofrobotics.com/" target="_blank" rel="noopener noreferrer">DOF Robotics</a> for a year, had an internship in <a href="https://x.company/" target="_blank" rel="noopener noreferrer">X</a> - the Moonshot Factory and 2 different internships in <a href="https://about.google/" target="_blank" rel="noopener noreferrer">Google</a>, one of them being purely on machine learning.</p> </div> <div class="news"> <h2>News</h2> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Sep 1, 2024</th> <td> Started my PhD program in Computer Science at NYU, Courant Institute of Mathematical Sciences! I will be working on Robot Learning for Dexterous Manipulation, advised by Lerrel Pinto. </td> </tr> <tr> <th scope="row">Sep 25, 2023</th> <td> We just published our latest project <a href="https://arxiv.org/abs/2309.12300" target="_blank" rel="noopener noreferrer">TAVI (See to Touch: Learning Tactile Dexterity through Visual Incentives)</a>! It got accepted to <strong>ICRA 2024</strong>. TAVI learns an online visuo-tactile policy using image only guidance. Learned policy can adapt to new environments under only 1 hour of training! For more check out <a href="https://see-to-touch.github.io/" target="_blank" rel="noopener noreferrer">website</a> and <a href="https://github.com/irmakguzey/see-to-touch" target="_blank" rel="noopener noreferrer">code</a>. </td> </tr> <tr> <th scope="row">Mar 22, 2023</th> <td> <a href="https://tactile-dexterity.github.io/" target="_blank" rel="noopener noreferrer">T-Dex (Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play)</a> got accepted to <strong>CoRL 2023</strong>! This is the first conference publication of mine as the main author. T-Dex is a dexterous manipulation framework that uses tactile observations. Tactile encoders are trained using SSL objectives on a tactile play data. </td> </tr> <tr> <th scope="row">Oct 12, 2022</th> <td> <a href="https://holo-dex.github.io/" target="_blank" rel="noopener noreferrer">Holo-Dex: Teaching Dexterity with Immersive Mixed Reality</a> got accepted to <strong>ICRA 2023</strong>! This is the first publication of my master’s and I had a blast working on this project with my colleagues. Holo-Dex is also out on <a href="https://arxiv.org/abs/2210.06463" target="_blank" rel="noopener noreferrer">Arxiv</a> now. </td> </tr> <tr> <th scope="row">Sep 1, 2021</th> <td> Started my master’s program in Computer Science in NYU, Courant Institute of Mathematical Sciences! </td> </tr> <tr> <th scope="row">Sep 1, 2021</th> <td> Ended my one year of full-time working experience as a robotics engineer in <a href="https://dofrobotics.com/" target="_blank" rel="noopener noreferrer">DOF robotics</a> to start my master’s program. Worked on implementing an autonomous guided vehicle (AGV) using ROS. </td> </tr> <tr> <th scope="row">Nov 30, 2020</th> <td> Ended my internship as a Machine Learning Engineer in <a href="https://about.google/" target="_blank" rel="noopener noreferrer">Google, Paris</a>. My main work was focused on improving Youtube’s annotation system by feature engineering. </td> </tr> <tr> <th scope="row">Aug 15, 2020</th> <td> Was awarded with a <strong>Fulbright scholarship</strong> on Computer Science. Was one of the 3 people who was granted a Fulbright scholarship in Computer Science in Turkey. </td> </tr> <tr> <th scope="row">Jun 2, 2020</th> <td> My first publication <a href="https://motionpredictionicra2020.github.io/posters/lhmp2020_guzey_paper.pdf" target="_blank" rel="noopener noreferrer">Human Motion Prediction with Graph Neural Networks</a> was accepted to <a href="https://motionpredictionicra2020.github.io/" target="_blank" rel="noopener noreferrer">Long-Term Human Motion Prediction workshop of ICRA 2020</a> !. </td> </tr> <tr> <th scope="row">Sep 10, 2019</th> <td> Ended my internship as a Software Engineer in <a href="https://x.company/" target="_blank" rel="noopener noreferrer">X, the Moonshot Factory</a>. </td> </tr> <tr> <th scope="row">Aug 20, 2018</th> <td> Ended my internship as a Software Engineer in <a href="https://about.google/" target="_blank" rel="noopener noreferrer">Google Zurich</a>! Worked on integrating UEFI Extraction’s results into the pipelines of malware detection systems. </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected Publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/hudor.gif"></div> <div id="guzey2024hudor" class="col-sm-8"> <div class="title">HuDOR: Bridging the Human to Robot Dexterity Gap through Object-Oriented Rewards</div> <div class="author"> <em>Irmak Guzey</em>, Yinlong Dai, Georgy Savva, Raunaq Bhirange, and Lerrel Pinto</div> <div class="periodical"> <em>(Under Review)</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/hudor.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://object-rewards.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Training robots directly from human videos is an emerging area in robotics and computer vision. While there has been notable progress with two-fingered grippers, learning autonomous tasks without teleoperation remains a difficult problem for multi-fingered robot hands. A key reason for this difficulty is that a policy trained on human hands may not directly transfer to a robot hand with a different morphology. In this work, we present HUDOR, a technique that enables online fine-tuning of the policy by constructing a reward function from the human video. Importantly, this reward function is built using object-oriented rewards derived from off-the-shelf point trackers, which allows for meaningful learning signals even when the robot hand is in the visual observation, while the human hand is used to construct the reward. Given a single video of human solving a task, such as gently opening a music box, HUDOR allows our four-fingered Allegro hand to learn this task with just an hour of online interaction. Our experiments across four tasks, show that HUDOR outperforms alternatives with an average of 4× improvement.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/tavi.gif"></div> <div id="guzey2023see" class="col-sm-8"> <div class="title">See to Touch: Learning Tactile Dexterity through Visual Incentives</div> <div class="author"> <em>Irmak Guzey</em>, Yinlong Dai, Ben Evans, Soumith Chintala, and Lerrel Pinto</div> <div class="periodical"> <em>ICRA 2024</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2309.12300" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/tavi.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://see-to-touch.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise, contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing fails to provide adequate cues for reasoning about objects’ spatial configurations, limiting the ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn visual representations. Next, we construct a reward function using these visual representations through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement learning on our robot to optimize tactile-based policies that maximize the visual reward. On six challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects, TAVI achieves a success rate of 73% using our four-fingered Allegro robot hand. The increase in performance is 108% higher than policies using tactile and vision-based rewards and 135% higher than policies without tactile observational input.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/tdex.gif"></div> <div id="guzey2023dexterity" class="col-sm-8"> <div class="title">Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play</div> <div class="author"> <em>Irmak Guzey</em>, Ben Evans, Soumith Chintala, and Lerrel Pinto</div> <div class="periodical"> <em>CoRL 2023</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2303.12076" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/tdex.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://tactile-dexterity.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics. Most prominent work in this area focuses on learning controllers or policies that either operate on visual observations or state estimates derived from vision. However, such methods perform poorly on fine-grained manipulation tasks that require reasoning about contact forces or about objects occluded by the hand itself. In this work, we present T-Dex, a new approach for tactile-based dexterity, that operates in two phases. In the first phase, we collect 2.5 hours of play data, which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional tactile readings to a lower-dimensional embedding. In the second phase, given a handful of demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based dexterity models outperform purely vision and torque-based models by an average of 1.7X. Finally, we provide a detailed analysis on factors critical to T-Dex including the importance of play data, architectures, and representation learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/holo-dex.gif"></div> <div id="arunachalam2022holo" class="col-sm-8"> <div class="title">Holo-Dex: Teaching Dexterity with Immersive Mixed Reality</div> <div class="author"> Sridhar Pandian Arunachalam, <em>Irmak Guzey</em>, Soumith Chintala, and Lerrel Pinto</div> <div class="periodical"> <em>ICRA 2023</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2210.06463" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/holodex.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="holo-dex.github.io" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>A fundamental challenge in teaching robots is to provide an effective interface for human teachers to demonstrate useful skills to a robot. This challenge is exacerbated in dexterous manipulation, where teaching high-dimensional, contact-rich behaviors often require esoteric teleoperation tools. In this work, we present HOLO-DEX, a framework for dexterous manipulation that places a teacher in an immersive mixed reality through commodity VR headsets. The high-fidelity hand pose estimator onboard the headset is used to teleoperate the robot and collect demonstrations for a variety of generalpurpose dexterous tasks. Given these demonstrations, we use powerful feature learning combined with non-parametric imitation to train dexterous skills. Our experiments on six common dexterous tasks, including in-hand rotation, spinning, and bottle opening, indicate that HOLO-DEX can both collect high-quality demonstration data and train skills in a matter of hours. Finally, we find that our trained skills can exhibit generalization on objects not seen in training.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/hmpwgnn.gif"></div> <div id="guzey2020lhmp" class="col-sm-8"> <div class="title">Human Motion Prediction With Graph Neural Networks</div> <div class="author"> <em>Irmak Guzey</em>, Ahmet E. Tekden, Evren Samur, and Emre Ugur</div> <div class="periodical"> <em></em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://motionpredictionicra2020.github.io/posters/lhmp2020_guzey_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/lhmp2020_guzey_paper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this work, we propose to use graph neural networks, propagation networks in particular, to investigate the problem of modelling full-body motion. The body parts and the relations between them are encoded as the nodes of a graph and edges between these nodes. How the nodes are related to each other is learned, and how the effects of multiple nodes on each node should be accumulated is computed in graph structure.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%69%72%6D%61%6B%67%75%7A%65%79@%6E%79%75.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://github.com/irmakguzey" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/irmak-guzey-6a9010175" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/irmakkguzey" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> Shoot me an email to chat! :) </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Irmak Guzey. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>