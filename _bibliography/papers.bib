---
---

@string{aps = {American Physical Society,}}


@article{zorin2024ruka,
  title={RUKA: Rethinking the Design of Humanoid Hands with Learning},
  author={Zorin*, Anya and Guzey*, Irmak and Yan, Billy and Iyer, Aadhithya and Kondrich, Lisa and Bhattasali, Nikhil X. and Pinto, Lerrel},
  abstract={This work presents RUKA, a tendon-driven humanoid hand that is compact,
  affordable, and capable. Made from 3D-printed parts and off-the-shelf components,
  RUKA has 5 fingers with 15 underactuated degrees of freedom enabling diverse human-like grasps.
  Its tendon-driven actuation allows powerful grasping in a compact, human-sized form factor.
  To address control challenges, we learn joint-to-actuator and fingertip-to- actuator models
  from motion-capture data collected by the MANUS glove, leveraging the hand's morphological accuracy.
  Extensive evaluations demonstrate RUKA's superior reachability, durability, and strength compared
  to other robotic hands. Tele- operation tasks further showcase RUKA's dexterous movements.
  The open-source design and assembly instructions of RUKA, code, and data are available on our website.},
  pdf={ruka.pdf},
  website={https://ruka-hand.github.io/},
  preview={ruka.gif},
  selected={true},
  journal={RSS 2025},
  year={2025}
}


@article{guzey2024hudor,
  title={HuDOR: Bridging the Human to Robot Dexterity Gap through Object-Oriented Rewards},
  author={Guzey, Irmak and Dai, Yinlong and Savva, Georgy and Bhirange, Raunaq and Pinto, Lerrel},
  abstract={Training robots directly from human videos is an emerging area in robotics and computer vision. 
  While there has been notable progress with two-fingered grippers, learning autonomous tasks without 
  teleoperation remains a difficult problem for multi-fingered robot hands. A key reason 
  for this difficulty is that a policy trained on human hands may not directly transfer to a robot hand with a different 
  morphology. In this work, we present HUDOR, a technique that enables online fine-tuning of the policy by constructing 
  a reward function from the human video. Importantly, this reward function is built using object-oriented rewards derived 
  from off-the-shelf point trackers, which allows for meaningful learning signals even when the robot hand is in the visual
  observation, while the human hand is used to construct the reward. Given a single video of human solving a task, such
  as gently opening a music box, HUDOR allows our four-fingered Allegro hand to learn this task with just an hour of
  online interaction. Our experiments across four tasks, show
  that HUDOR outperforms alternatives with an average of 4Ã— improvement.},
  pdf={hudor.pdf},
  website={https://object-rewards.github.io/},
  preview={hudor.gif},
  selected={true},
  journal={ICRA 2025},
  year={2024}
}

@article{oyer2023openteach,
  title={OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation},
  author={Iyer, Aadhithya and Peng, Zhuoran and Dai, Yinlong and Guzey, Irmak and Haldar, Siddhant and Chintala, Soumith and Pinto, Lerrel},
  abstract={Open-sourced, user-friendly tools form the bedrock of scientific advancement across disciplines.
  The widespread adoption of data-driven learning has led to remarkable progress in multi-fingered dexterity, 
  bimanual manipulation, and applica- tions ranging from logistics to home robotics. However, existing data 
  collection platforms are often proprietary, costly, or tailored to specific robotic morphologies. 
  We present OPEN TEACH, a new teleoperation system leveraging VR headsets to immerse users in mixed 
  reality for intuitive robot control. Built on the affordable Meta Quest 3, which costs $500, OPEN TEACH 
  enables real- time control of various robots, including multi-fingered hands, bimanual arms, and mobile 
  manipulators, through an easy-to- use app. Using natural hand gestures and movements, users can manipulate 
  robots at up to 90Hz with smooth visual feedback and interface widgets offering closeup environment views.
   We demonstrate the versatility of OPEN TEACH across 38 tasks on different robots. A comprehensive user 
   study indicates significant improvement in teleoperation capability over the AnyTeleop framework. 
   Further experiments exhibit that the collected data is compatible with policy learning on 10 dexterous
    and contact- rich manipulation tasks. Currently supporting Franka, xArm, Jaco, Allegro, and Hello Stretch 
    platforms, OPEN TEACH is fully open-sourced to promote broader adoption.},
  website={https://open-teach.github.io},
  preview={openteach.png},
  selected={true},
  journal={CORL 2024},
  year={2024}
}

@article{guzey2023see,
  title={See to Touch: Learning Tactile Dexterity through Visual Incentives},
  author={Guzey, Irmak and Dai, Yinlong and Evans, Ben and Chintala, Soumith and Pinto, Lerrel},
  url={https://arxiv.org/abs/2309.12300},
  html={https://arxiv.org/abs/2309.12300},
  abstract={Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise,
  contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing
  fails to provide adequate cues for reasoning about objects' spatial configurations, limiting the
  ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation
  from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing
  dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn
  visual representations. Next, we construct a reward function using these visual representations
  through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement
  learning on our robot to optimize tactile-based policies that maximize the visual reward. On six
  challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects,
  TAVI achieves a success rate of 73\% using our four-fingered Allegro robot hand. The increase in
  performance is 108\% higher than policies using tactile and vision-based rewards and 135\% higher
  than policies without tactile observational input.},
  pdf={tavi.pdf},
  website={https://see-to-touch.github.io/},
  preview={tavi.gif},
  selected={true},
  journal={ICRA 2024},
  year={2023}
}

@article{guzey2023dexterity,
  title={Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play},
  author={Guzey, Irmak and Evans, Ben and Chintala, Soumith and Pinto, Lerrel},
  url={https://arxiv.org/abs/2303.12076},
  html={https://arxiv.org/abs/2303.12076},
  abstract={Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics.
  Most prominent work in this area focuses on learning controllers or policies that either operate
  on visual observations or state estimates derived from vision. 
  However, such methods perform poorly on fine-grained manipulation tasks that require
  reasoning about contact forces or about objects occluded by the hand itself.
  In this work, we present T-Dex, a new approach for tactile-based dexterity,
  that operates in two phases. In the first phase, we collect 2.5 hours of play data,
  which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional
  tactile readings to a lower-dimensional embedding. In the second phase, given a handful of
  demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile
  observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based
  dexterity models outperform purely vision and torque-based models by an average of 1.7X.
  Finally, we provide a detailed analysis on factors critical to T-Dex including the importance of
  play data, architectures, and representation learning.},
  pdf={tdex.pdf},
  website={https://tactile-dexterity.github.io/},
  preview={tdex.gif},
  selected={true},
  journal={CoRL 2023},
  year={2023}
}

@article{arunachalam2022holo,
  title={Holo-Dex: Teaching Dexterity with Immersive Mixed Reality},
  author={Arunachalam, Sridhar Pandian and Guzey, Irmak and Chintala, Soumith and Pinto, Lerrel},
  url={https://arxiv.org/abs/2210.06463},
  html={https://arxiv.org/abs/2210.06463},
  abstract={A fundamental challenge in teaching robots is to
provide an effective interface for human teachers to demonstrate useful skills to a robot. This challenge is exacerbated
in dexterous manipulation, where teaching high-dimensional,
contact-rich behaviors often require esoteric teleoperation tools.
In this work, we present HOLO-DEX, a framework for dexterous manipulation that places a teacher in an immersive mixed
reality through commodity VR headsets. The high-fidelity hand
pose estimator onboard the headset is used to teleoperate the
robot and collect demonstrations for a variety of generalpurpose dexterous tasks. Given these demonstrations, we use
powerful feature learning combined with non-parametric imitation to train dexterous skills. Our experiments on six common
dexterous tasks, including in-hand rotation, spinning, and bottle
opening, indicate that HOLO-DEX can both collect high-quality
demonstration data and train skills in a matter of hours. Finally,
we find that our trained skills can exhibit generalization on
objects not seen in training.},
  pdf={holodex.pdf},
  year={2022},
  website={holo-dex.github.io},
  preview={holo-dex.gif},
  journal={ICRA 2023},
  selected={true}
}

@article{guzey2020lhmp,
  title={Human Motion Prediction With Graph Neural Networks},
  author={Guzey, Irmak and Tekden, Ahmet E. and Samur, Evren and Ugur, Emre},
  url={https://motionpredictionicra2020.github.io/posters/lhmp2020_guzey_paper.pdf},
  abstract={In this work, we propose to use graph neural networks, propagation networks in particular, to investigate the problem of modelling full-body motion.
  The body parts and the relations between them are encoded as the nodes of a graph and edges between these nodes. 
  How the nodes are related to each other is learned, and how the effects of multiple nodes on each node should be accumulated is computed in graph structure.},
  pdf={lhmp2020_guzey_paper.pdf},
  html={https://motionpredictionicra2020.github.io/posters/lhmp2020_guzey_paper.pdf},
  year={2020},
  preview={hmpwgnn.gif},
  selected={true}
}
