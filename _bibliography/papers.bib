---
---

@string{aps = {American Physical Society,}}

@article{guzey2023see,
  title={See to Touch: Learning Tactile Dexterity through Visual Incentives},
  author={Guzey, Irmak and Dai, Yinlong and Evans, Ben and Chintala, Soumith and Pinto, Lerrel},
  url={https://arxiv.org/abs/2309.12300},
  html={https://arxiv.org/abs/2309.12300},
  abstract={Equipping multi-fingered robots with tactile sensing is crucial for achieving the precise,
  contact-rich, and dexterous manipulation that humans excel at. However, relying solely on tactile sensing
  fails to provide adequate cues for reasoning about objects' spatial configurations, limiting the
  ability to correct errors and adapt to changing situations. In this paper, we present Tactile Adaptation
  from Visual Incentives (TAVI), a new framework that enhances tactile-based dexterity by optimizing
  dexterous policies using vision-based rewards. First, we use a contrastive-based objective to learn
  visual representations. Next, we construct a reward function using these visual representations
  through optimal-transport based matching on one human demonstration. Finally, we use online reinforcement
  learning on our robot to optimize tactile-based policies that maximize the visual reward. On six
  challenging tasks, such as peg pick-and-place, unstacking bowls, and flipping slender objects,
  TAVI achieves a success rate of 73\% using our four-fingered Allegro robot hand. The increase in
  performance is 108\% higher than policies using tactile and vision-based rewards and 135\% higher
  than policies without tactile observational input.},
  pdf={tavi.pdf},
  website={https://see-to-touch.github.io/},
  preview={tavi.gif},
  selected={true},
  journal={arXiv preprint arXiv:2309.12300},
  year={2023}
}

@article{guzey2023dexterity,
  title={Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play},
  author={Guzey, Irmak and Evans, Ben and Chintala, Soumith and Pinto, Lerrel},
  url={https://arxiv.org/abs/2303.12076},
  html={https://arxiv.org/abs/2303.12076},
  abstract={Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics.
  Most prominent work in this area focuses on learning controllers or policies that either operate
  on visual observations or state estimates derived from vision. 
  However, such methods perform poorly on fine-grained manipulation tasks that require
  reasoning about contact forces or about objects occluded by the hand itself.
  In this work, we present T-Dex, a new approach for tactile-based dexterity,
  that operates in two phases. In the first phase, we collect 2.5 hours of play data,
  which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional
  tactile readings to a lower-dimensional embedding. In the second phase, given a handful of
  demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile
  observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based
  dexterity models outperform purely vision and torque-based models by an average of 1.7X.
  Finally, we provide a detailed analysis on factors critical to T-Dex including the importance of
  play data, architectures, and representation learning.},
  pdf={tdex.pdf},
  website={https://tactile-dexterity.github.io/},
  preview={tdex.gif},
  selected={true},
  journal={CoRL 2023},
  year={2023}
}

@article{arunachalam2022holo,
  title={Holo-Dex: Teaching Dexterity with Immersive Mixed Reality},
  author={Arunachalam, Sridhar Pandian and Guzey, Irmak and Chintala, Soumith and Pinto, Lerrel},
  url={https://arxiv.org/abs/2210.06463},
  html={https://arxiv.org/abs/2210.06463},
  abstract={A fundamental challenge in teaching robots is to
provide an effective interface for human teachers to demonstrate useful skills to a robot. This challenge is exacerbated
in dexterous manipulation, where teaching high-dimensional,
contact-rich behaviors often require esoteric teleoperation tools.
In this work, we present HOLO-DEX, a framework for dexterous manipulation that places a teacher in an immersive mixed
reality through commodity VR headsets. The high-fidelity hand
pose estimator onboard the headset is used to teleoperate the
robot and collect demonstrations for a variety of generalpurpose dexterous tasks. Given these demonstrations, we use
powerful feature learning combined with non-parametric imitation to train dexterous skills. Our experiments on six common
dexterous tasks, including in-hand rotation, spinning, and bottle
opening, indicate that HOLO-DEX can both collect high-quality
demonstration data and train skills in a matter of hours. Finally,
we find that our trained skills can exhibit generalization on
objects not seen in training.},
  pdf={holodex.pdf},
  year={2022},
  website={holo-dex.github.io},
  preview={holo-dex.gif},
  journal={ICRA 2023},
  selected={true}
}

@article{guzey2020lhmp,
  title={Human Motion Prediction With Graph Neural Networks},
  author={Guzey, Irmak and Tekden, Ahmet E. and Samur, Evren and Ugur, Emre},
  url={https://motionpredictionicra2020.github.io/posters/lhmp2020_guzey_paper.pdf},
  abstract={In this work, we propose to use graph neural networks, propagation networks in particular, to investigate the problem of modelling full-body motion.
  The body parts and the relations between them are encoded as the nodes of a graph and edges between these nodes. 
  How the nodes are related to each other is learned, and how the effects of multiple nodes on each node should be accumulated is computed in graph structure.},
  pdf={lhmp2020_guzey_paper.pdf},
  html={https://motionpredictionicra2020.github.io/posters/lhmp2020_guzey_paper.pdf},
  year={2020},
  preview={hmpwgnn.gif},
  selected={true}
}
